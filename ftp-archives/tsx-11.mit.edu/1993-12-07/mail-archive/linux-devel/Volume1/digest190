From:     Digestifier <Linux-Development-Request@senator-bedfellow.mit.edu>
To:       Linux-Development@senator-bedfellow.mit.edu
Reply-To: Linux-Development@senator-bedfellow.mit.edu
Date:     Tue, 26 Oct 93 08:13:08 EDT
Subject:  Linux-Development Digest #190

Linux-Development Digest #190, Volume #1         Tue, 26 Oct 93 08:13:08 EDT

Contents:
  Bug in 'hd=' boot parm for 0.99pl13 (Dave Clemans)
  Re: ugly name for core dumps (core.imagename) -> patch for "img.core" (Bill C. Riemers)
  Re: meta-fs device idea (Jacques Gelinas)
  Re: Lots of zombies with ALPHA-pl13j (Joerg Schlaeger)
  Re: IDE/interrupt latency patch (Ove Ewerlid)
  Re: Slowness in scsi disk access (Steven A. Reisman)
  Re: Slowness in scsi disk access (Michael O'Reilly)
  Re: Slowness in scsi disk access (Michael O'Reilly)
  Re: Slowness in scsi disk access (Eric Youngdale)
  Re: IDE/interrupt latency patch (Michael O'Reilly)
  lib4.4.4 & linuxpl13j (jschief@finbol.toppoint.de)
  Re: lib4.4.4 & linuxpl13j (Linus Torvalds)
  Re: Bugs in Quota-Patches (SLS 1.03, 0.99pl12) (Kai Kretschmann)
  Re: Andrew File System (Charles T Wilson -- Personal Account)

----------------------------------------------------------------------------

From: dclemans@$DOMAIN (Dave Clemans)
Subject: Bug in 'hd=' boot parm for 0.99pl13
Date: 25 Oct 1993 22:42:24 GMT
Reply-To: dave_clemans@mentorg.com

I've think I've found a bug in the support for the 'hd=' boot parameter
(that provides disk geometry info to the IDE driver) in the 0.99pl13
kernel. The net effect of the bug is that in most circumstances the
'hd=' parameter will be completely ignored.

The problem is in the hd_geninit routine. If the code that automatically
tries to get the disk geometry is compiled in, it's executed unconditionally.
Thus, any information previously put into those tables by hd_setup (because
'hd=' was passed in from LILO) is gone.

Where this causes problems is for the systems where the automatic
geometry code doesn't work, such as IBM PS/1's, ValuePoints, etc.

The obvious fix is to make the automatic geometry code conditional,
so that it doesn't run if hd_setup has been called.

dgc


------------------------------

From: bcr@bohr.physics.purdue.edu (Bill C. Riemers)
Subject: Re: ugly name for core dumps (core.imagename) -> patch for "img.core"
Date: 25 Oct 93 21:38:46 GMT

In article <RFRANKEL.93Oct25111636@obelix.obelix.us.oracle.com> rfrankel@us.oracle.com (Rick Frankel) writes:
>From: bcr@bohr.physics.purdue.edu (Bill C. Riemers)
>BR> I know I am.  The following patch makes the command "make clean"
><lots deleted>
>BR>    --> I've modified make clean to remove core.* files, instead of core.
>All make `clean' will have to be modified to rm *.o, etc then execute
>the `rmcore' script.

Fortunately there are not files in the /usr/src/linux tree that are named
core.c or core.h, so this is not a problem in this case...  However in
general an inconvient wild card of:

CORE = core.{[ab,d-g,j-r,t-z,AB,D-G,J-R,T-Z,0-9]*,[chsCHS]?*}

        rm -f $(CORE)

needs to be used...  I know I would much prefere to have core as an
extention to going through all these acrobatics in a "rm" command.
 
                               Bill


------------------------------

From: jack@solucorp.qc.ca (Jacques Gelinas)
Subject: Re: meta-fs device idea
Date: Mon, 25 Oct 93 21:45:10 GMT

jeremy@suite.sw.oz.au (Jeremy Fitzhardinge) writes:

>In <RIDEAU.93Oct21172751@prao.ens.fr> rideau@ens.fr (Francois-Rene Rideau) writes:
>>  The thing is an filter device (a la loop device, or in fs mounting options)
>>over filesystems, that would allow --longnames--access rights--automatic
>>compression/decompression--user-level mounting-- (well, at least longnames to
>>begin with) over any filesystem transparently through a (hidden by default)
>>[optional .]filterfs[optional .]rc (or anything) description file ?

>Yes and no.  I wrote a filesystem driver "userfs" which allows user
>processes to be mounted and act like filesystems.  This would let
>you do everything you want to here -- but it would have to be
>written.

>userfs lets the process control most aspects of a filesystem.  The
>kernel calls the filesystem, which in turn packages up the call
>and sends it off to the process.  The process does whatever it
>needs to do and sends back a result.

>This is all pretty low-level - there is no existing facility to do
>all the processing you talk about.  Most particularly, it is a
>complete filesystem itself, with no option to overlay upon an
>existing one.  Something similar can be done with "ifs" -- the
>inhereting filesystem that allows filesystems to be stacked on top
>of each other.  ifs in combination with userfs could be the basis
>of what you're looking for.

On another note, I am about to publish UMSDOS. This is a file system
which run piggyback on top of the standard linux MSDOS FS. UMSDOS
is a Unix-like file system with all the goodies (long name, permissions,
symlink, hard link, owner, etc ...). With UMSDOS it is possible
to run Linux completly inside a DOS FAT partition. I have solve
the swapping and demand loading problem also. I intend to go ALPHA
with this within a week or two.


It should allow anyone to install Linux without the problem of repartionning
the disk.

Any ALPHA tester interested ?

-- 

========================================================
Jacques Gelinas (jacques@solucorp.qc.ca)
Maintainer of US4BINR jacques@us4binr.login.qc.ca

------------------------------

From: joergs@toppoint.de (Joerg Schlaeger)
Subject: Re: Lots of zombies with ALPHA-pl13j
Date: 26 Oct 1993 01:38:25 +0200

chris@csvax1.ucc.ie (Chris Higgins - System Administrator) writes:

>>>Anyone else had a problem with loads of zombies forming with pl13j?
>>There is a reason for calling it Alpha you know.  Meaning, it probably
>That particular bug seems to be removed from pl13k..

This didnot happend in pl13h, so it might be good to know that
it is a problem with the kernel and not the libraries4.4.4 or ??

Joerg

-- 
Joerg Schlaeger         joergs@picard.toppoint.de
24113 Kiel 1             Tel.: ++49 431 682210
(to be faster with the /2)

------------------------------

From: ewerlid@frej.teknikum.uu.se (Ove Ewerlid)
Subject: Re: IDE/interrupt latency patch
Date: 25 Oct 1993 23:50:23 GMT



It would be interesting to find out what kind of systems the
users that experienced disk corruption used/use.
I never saw any hints on this around about the time Linus
experimented with the interrupt lattency in hd.c.
Is there such information around?
Are any of those users reading this?

BTW: What machine setup are you using Jim?
I've been running with patches similar to those you posted
without any problems for some months now.
I've a 486DX2@66, cheap IDE controller and Conner drives. 


------------------------------

From: sar@bee.beehive.mn.org (Steven A. Reisman)
Subject: Re: Slowness in scsi disk access
Date: 25 Oct 93 22:41:33 GMT

Eric Youngdale (eric@tantalus.nrl.navy.mil) wrote:
:       This thread started on the scsi channel, because people constantly
: complain about how slow the scsi disks are under linux are relative to DOS.  I
: do not know if there is a similar difference for IDE disks, but I have been
: thinking about this, and I have a few ideas that I wanted to share.  These are
: all just ideas, of course, and if anyone else has any thoughts to contribute
: I would be interested in hearing them.  Once I have a pretty solid
: understanding of where the delays come from, it will be easier to speed things
: up. 

<lots of insight deleted.....


: -Eric

Why isn't a linked list of available buffers kept.  Then would be no search
time involved.  Pull the first available buffer off the list and update the
buffer pointer, what could be simpler?

======
Steven A. Reisman                                     sar@bee.beehive.mn.org      
Afton, MN  55001                                              (612) 436-7125

-- 
======
Steven A. Reisman                                     sar@bee.beehive.mn.org      
Afton, MN  55001                                              (612) 436-7125

------------------------------

From: oreillym@tartarus.uwa.edu.au (Michael O'Reilly)
Subject: Re: Slowness in scsi disk access
Date: 26 Oct 1993 04:43:19 GMT

Eric Youngdale (eric@tantalus.nrl.navy.mil) wrote:

[ much impressive prose deleted. ]

:       2) If we discover that all of the buffers in the buffer cache are
: dirty, we call sync.  The problem with doing this is that the process that is
: dirtying buffers gets stuck doing the sync, and the user program will not run
: again until the sync is complete.  The request queue can hold at
: most 4Mb worth 
: of buffers, so the user process can get stalled for a considerable amount of
: time.  I wonder whether it would be a good idea to wake up the update process
: once something like 50-75% of the buffers are dirty so that the writing can
: take place at the same time that the user processes are dirtying
: more buffers. 
: The reason that this is important is that the disk writing code can be quite
: lightweight, and it would be quite easy to have the disk being written at the
: same time that a different process is dirtying more buffers.  This
: may require 
: some means by which the update process would register itself to the
: kernel (in 
: theory we could do it based upon the filename alone, but this seems
: unclean to 
: me), and I presume that sending a SIGALRM would be all that was
: required to get 
: things going.  You would also have to clear the pointer to the update task if
: this process ever goes away for any reason.

Possibly better would be to tell the init task. This will always be
around, it will always be alive, and it will always be in a known
location.
        cons
                required support from init
        pro
                not fatal if the support isn't there.

: -Eric

Points:
1) will you release the code for this emulator? sounds like something
fun to play with while avoiding study....
2) little nitpick. Could you keep your line lengths under 72 columns?
so that quoting doesn't stuff things.

3) curiosity: How well does the hashing work in searching for buffers?
Got any stats on hit/miss ratios ?? search lengths?

Michael.

------------------------------

From: oreillym@tartarus.uwa.edu.au (Michael O'Reilly)
Subject: Re: Slowness in scsi disk access
Date: 26 Oct 1993 04:43:25 GMT

Steven A. Reisman (sar@bee.beehive.mn.org) wrote:

: Why isn't a linked list of available buffers kept.  Then would be no search
: time involved.  Pull the first available buffer off the list and update the
: buffer pointer, what could be simpler?


Hmm. At this point it's probably a good time to tell you to read the
source. :). To be simplistic, this IS the way the buffers are kept. As
a list of available buffers... but...

The buffer handling is complicated a number of ways: a buffer may be
dirty, locked, the wrong size, shared by a code page, used recently,
etc etc.

All these are reasons why it shouldn't be reused next. The problem is
that most of these conditions can change freqently, and damn near at
random. The other point is that what happens when ALL the buffers are
one of {dirty,locked,shared,wrong size}??

Look at linux/fs/buffer.c:getblk(). This is really the heart of the
system. Hopefully, this routine shouldn't take very long most of the
time. Anyone got some stats on average search length??

Study, ponder, and feel awe at the wisdom of the Linus. :)

Michael.

------------------------------

From: eric@tantalus.nrl.navy.mil (Eric Youngdale)
Subject: Re: Slowness in scsi disk access
Date: Tue, 26 Oct 1993 04:28:54 GMT

In article <1993Oct25.224133.1441@bee.beehive.mn.org> sar@bee.beehive.mn.org (Steven A. Reisman) writes:
>Why isn't a linked list of available buffers kept.  Then would be no search
>time involved.  Pull the first available buffer off the list and update the
>buffer pointer, what could be simpler?

        It is not quite that simple.  It turns out that the free list consists
of the entire buffer cache, and they generally appear in a last-used first
order.  You cannot tell ahead of time which buffers you will need again, so you
cannot declare that some buffers are available and others are not.  There are,
of course buffers that are in active use by a program, but usually there are
not that many, and for our discussion purposes we can ignore these.

        To illustrate this, take the example of when you compile the kernel.
The gcc compiler is stored in the buffer cache after you invoke it the first
time, and subseqquent invocations are satisfied through the buffer cache
without a need to go back to disk.  Inbetween compiling something the buffers
are all unused, but because the buffers appear in the free list with the most
recently used buffers at the bottom, you will not be reusing buffers that
contain portions of the compiler when you run something else, the assembler for
example.

        All well and good, you say.  Why can't we just take the first buffer
from the free list??? The problem cones up because the free list contains both
clean and dirty buffers (i.e. buffers that correspond to a file that has been
written.  If we were to try and reuse a dirty buffer, we would have to write
the contents to disk first before we could reuse it, so we obviously prefer to
snag a clean buffer instead.  Once the sync() function is called, then these
buffers are all clean, and we could reuse them.  In principle we could keep the
free and dirty buffers in separate lists, and this might serve to help cut down
on some of the overhead, but once the buffers are written and no longer dirty,
then it would be hard to know where to insert them on the free list.

        As I mentioned, I copied portions of the kernel to essentially
benchmark the buffer cache.  As long as we are pulling buffers from the top of
the free list, I get theoretical throughputs of something like 10-15Mb/sec.
This basically tells me that as long as we do not have to search the free list
for a suitable buffer that there is no bottleneck in the buffer cache.
As I already pointed out, there are cases where this is not the case, and then
we fall flat on our faces.

        There is still something that I do not understand, and that is why the
writing speed is so low.  If we can fill buffers in the buffer cache at
10Mb/sec, and the low-level scsi code can push data through at 2Mb/sec, the
question is why do end users see something like 300Kb/sec?  Starting a sync
when only half of the buffers are dirty makes sense, but my model does not
uncover any shortcomings in the buffer cache code when we are furiously writing
data.  As far as I can tell, the most likely explanation is that the request
queueing mechanism is the bottleneck here, but I have not identified a
reasonable explanation of why this might be the case.  Stay tuned for further
developments....

        I should mention in passing that the 10-15Mb/sec figure comes from
using all of the kernel code from block_[read,write] all the way through
ll_rw_block() until we reach a stub routine that serves as the request function
for my fake driver.  In fact, I am able to use block_dev.c, buffer.c and
ll_rw_block.c with only the most trivial of modifications (i.e. commenting out
cli() and sti()), so it is a pretty good indication of what the true
capabilities of the buffer cache are.  The only thing I am missing is the
overhead of switching in and out of kernel mode.  This could probably be
simulated by calling some kernel function that will return more or less
immediately, I suppose - I might toss this into the pot as well just for
completeness sake.

-Eric

-- 
"The woods are lovely, dark and deep.  But I have promises to keep,
And lines to code before I sleep, And lines to code before I sleep."

------------------------------

From: oreillym@tartarus.uwa.edu.au (Michael O'Reilly)
Subject: Re: IDE/interrupt latency patch
Date: 26 Oct 1993 05:18:17 GMT

Ove Ewerlid (ewerlid@frej.teknikum.uu.se) wrote:


: It would be interesting to find out what kind of systems the
: users that experienced disk corruption used/use.
: I never saw any hints on this around about the time Linus
: experimented with the interrupt lattency in hd.c.
: Is there such information around?
: Are any of those users reading this?

Well, my fairly normal machine with an 'alps' hard disk (that's the
company that manufactures it) suffered VERY badly from corruption.
Most of the problem is that the disk was manufactured with messydos in
mind, and the interrupt handling is sloppy to say the least. It is
rather found of sending a few more interrupts than necesary which
would result in the kernel reading before the disk was ready etc etc.

Normal result was reading a corrupt sector, and then writing it back,
or just simply having the write silently fail. It also was prone to
'locking' to the point where you had to power down to get access to
the drive again. (a reset did NOT work). The drive seemed to lock
while writing, usually on the superblock :( and of course, powering
down normally introduced further corruption.

Fun drive. These days I use #define HD_DELAY 300 to get it semi-
reliable..

machine is 486/33, 8 megs of memory, rest normal. drive is 200Meg IDE.

------------------------------

From: jschief@finbol.toppoint.de
Subject: lib4.4.4 & linuxpl13j
Date: Mon, 25 Oct 1993 15:47:33 GMT


Hi,
I just installed lib 4.4.4 and 99pl13j, and get a lot of zombies
started with whod, rnews, inews, ..., it seams to all that need
nameresolving from the named.  
This did not happend with 99pl13h .   

Any Ideas to find my fault ??,
where can I find a the new net-util0.28.tgz ??

Thanks
Joerg

-- 
Joerg Schlaeger                     jschief@finbol.toppoint.de
24113 Kiel                          Tel.: ++49 431 682210 (voice)
=================================================================
To be or what ?

------------------------------

From: torvalds@klaava.Helsinki.FI (Linus Torvalds)
Subject: Re: lib4.4.4 & linuxpl13j
Date: 26 Oct 1993 12:38:30 +0200

In article <1993Oct25.154733.133@finbol.toppoint.de>,
 <jschief@finbol.toppoint.de> wrote:
>
>I just installed lib 4.4.4 and 99pl13j, and get a lot of zombies
>started with whod, rnews, inews, ..., it seams to all that need
>nameresolving from the named.  
>This did not happend with 99pl13h .   
>
>Any Ideas to find my fault ??,

It's a bug in '13j' - fixed in '13k'. Sorry,

                Linus

------------------------------

From: kai@fix.kmk.rhein-main.de (Kai Kretschmann)
Subject: Re: Bugs in Quota-Patches (SLS 1.03, 0.99pl12)
Date: Mon, 25 Oct 1993 20:52:10 GMT

I have another bad error with enabled quota. Every time I get an
uucp login over a serial line the system freezes during the
logout procedure. If I log in using this line with any human id
everythings works ok. All logins on local vc's are OK, too.

I think it has something to do with id's below some value, I think
it was 100. These accounts doesn't get any limitation or so and there
might be an error. Perhaps someone already found it? The previously
posted patch didn't influence this behavior, BTW.

-- 
Kai Kretschmann,
  >>>   FidoNet:  2:248/312, 21:100/5729, 16:100/6006    <<<
  >>>   Internet: kai@fix.kmk.rhein-main.de              <<<
  >>>   FAX/BBS:  +49-6172-305379                        <<<

------------------------------

From: ctwilson@rock.concert.net (Charles T Wilson -- Personal Account)
Subject: Re: Andrew File System
Date: 26 Oct 1993 00:49:49 GMT

In article <1993Oct25.085410.20794@nessie.mcc.ac.uk>,
 <zzassgl@gl.mcc.ac.uk> wrote:
>Gary Keim (gk5g+@andrew.cmu.edu) wrote:
>: Excerpts from netnews.comp.os.linux.development: 22-Oct-93 Re: Andrew
>: File System Heikki Suonsivu@cs.hut.f (645) 
>
>: > AFS would be better than NFS, but I don't see what is the point of trying 
>: > to be compatible with AFS which isn't used anywhere (a major example of how 
>: > making something propretiary looses :-). 
>
>: % ls /afs 
>: alw.nih.gov              dsg.stanford.edu         nersc.gov 
>    ..........
>: dmsv.med.umich.edu       nd.edu                   wam.umd.edu 
>
>: That's quite a few nowhere's. 
>
>: -Gary Keim 
>: Andrew Consortium 
>
>
>And how many people at each site are locked out of the facilities because
>they are only available via AFS?  It's sad that educational sites decide to
>go with propretiary servers.

AFS volumes *are* available to nfs-mounted machines.  My employer uses a
mixture of both.  It's a matter of being granted access by an administrator
or the owner(s) of the volume(s) in question.

-- 
/-----------------------------------------------------------------------\
|  Tom Wilson                      |  "I can't complain, but sometimes  |
|  ctwilson@rock.concert.net       |   I still do."                     |
|                                  |                -Joe Walsh          |

------------------------------


** FOR YOUR REFERENCE **

The service address, to which questions about the list itself and requests
to be added to or deleted from it should be directed, is:

    Internet: Linux-Development-Request@NEWS-DIGESTS.MIT.EDU

You can send mail to the entire list (and comp.os.linux.development) via:

    Internet: Linux-Development@NEWS-DIGESTS.MIT.EDU

Linux may be obtained via one of these FTP sites:
    nic.funet.fi				pub/OS/Linux
    tsx-11.mit.edu				pub/linux
    sunsite.unc.edu				pub/Linux

End of Linux-Development Digest
******************************
